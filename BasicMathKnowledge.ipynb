{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### 数组是编程语言当中的概念，而张量和矩阵是数学当中的概念\n",
    "+ 标量(scalar)：一个标量就是一个单独的数(整数或实数)，不同于线性代数中研究的其他大部分对象(通常是多个数的数组)。\n",
    "在Python中的定义为:\n",
    "`x = 1`\n",
    "+ 向量(vector)：一个向量表示一组有序排列的数，通过次序中的索引我们能够找到每个单独的数。向量中的每个元素就是一个标量，向量中的第i个元素用表示。在Python中的定义为：\n",
    "`import numpy as np`\n",
    "`行向量`\n",
    "`a = np.array([1,2,3,4])`\n",
    "+ 矩阵(matrix)：矩阵是一个二维数组，其中的每一个元素由两个索引来决定 ，矩阵通常用加粗斜体的大写字母表示。我们可以将矩阵看做是一个二维的数据表，矩阵的每一行表示一个对象，每一列表示一个特征。在Python中的定义为：\n",
    "`import numpy as np`\n",
    "`张量`\n",
    "`a = np.array([[[1,2],[3,4]],[[5,6],[7,8]]])`\n",
    "+ 张量(tensor)：超过二维的数组，一般来说，一个数组中的元素分布在若干维坐标的规则网格中，被称为张量。如果一个张量是三维数组，那么我们就需要三个索引来决定元素的位置，张量通常用加粗的大写字母表示。机器学习当中的张量和数学当中的不太一样，机器学习中的张量一般就是指多维数组\n",
    "`import numpy as np`\n",
    "`张量`\n",
    "`a = np.array([[[1,2],[3,4]],[[5,6],[7,8]]])`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([5.]), tensor([6.]), tensor([1.5000]), tensor([9.]))"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标量由只有一个元素的张量表示\n",
    "import numpy as np\n",
    "import torch\n",
    "x = torch.tensor([3.0])\n",
    "y = torch.tensor([2.0])\n",
    "x + y, x * y, x / y, x**y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 1, 2, 3])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 你可以将向量视为标量值组成的列表\n",
    "x = torch.arange(4)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(3)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以通过张量的索引来访问任一元素\n",
    "x[3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 访问张量的长度\n",
    "len(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 只有一个轴的张量，形状只有一个元素\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11],\n         [12, 13, 14, 15],\n         [16, 17, 18, 19]]),\n tensor([[ 0,  4,  8, 12, 16],\n         [ 1,  5,  9, 13, 17],\n         [ 2,  6, 10, 14, 18],\n         [ 3,  7, 11, 15, 19]]))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过指定两个分量m和n来创建一个形状为mXn的矩阵\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "A, A.T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.],\n         [16., 17., 18., 19.]]),\n tensor([[ 0.,  2.,  4.,  6.],\n         [ 8., 10., 12., 14.],\n         [16., 18., 20., 22.],\n         [24., 26., 28., 30.],\n         [32., 34., 36., 38.]]))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 给定任何具有相同形状的任何两个向量，任何按元素二元运算的结果都将是相同形状的向量\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()\n",
    "A, A + B"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0.,   1.,   4.,   9.],\n        [ 16.,  25.,  36.,  49.],\n        [ 64.,  81., 100., 121.],\n        [144., 169., 196., 225.],\n        [256., 289., 324., 361.]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 两个矩阵的按元素乘法称为哈达玛积\n",
    "A * B"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 2,  3,  4,  5],\n          [ 6,  7,  8,  9],\n          [10, 11, 12, 13]],\n \n         [[14, 15, 16, 17],\n          [18, 19, 20, 21],\n          [22, 23, 24, 25]]]),\n torch.Size([2, 3, 4]))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15],\n        [16, 17, 18, 19]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定求和汇总的关键维度\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "A"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([40, 45, 50, 55]),\n tensor([ 6, 22, 38, 54, 70]),\n tensor(190),\n tensor(190))"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A.sum()==A.sum(axis=(0, 1))\n",
    "A.sum(axis=0), A.sum(axis=1), A.sum(), A.sum(axis=(0, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[20, 22, 24, 26],\n         [28, 30, 32, 34],\n         [36, 38, 40, 42],\n         [44, 46, 48, 50],\n         [52, 54, 56, 58]]),\n torch.Size([5, 4]))"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定求和汇总的关键维度，对某个维度求和可以理解就是消除某个维度\n",
    "A = torch.arange(40).reshape(2, 5, 4)\n",
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 40,  45,  50,  55],\n         [140, 145, 150, 155]]),\n torch.Size([2, 4]))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.],\n         [16., 17., 18., 19.]], dtype=torch.float64),\n tensor(9.5000, dtype=torch.float64),\n tensor(9.5000, dtype=torch.float64))"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一个与求和相关的量是平均值\n",
    "A = torch.arange(20, dtype=float).reshape(5, 4)\n",
    "A, A.mean(), A.sum() / A.numel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 8.,  9., 10., 11.], dtype=torch.float64),\n tensor([ 8.,  9., 10., 11.], dtype=torch.float64))"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定维度\n",
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 6.],\n         [22.],\n         [38.],\n         [54.],\n         [70.]], dtype=torch.float64),\n tensor([ 6., 22., 38., 54., 70.], dtype=torch.float64))"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算总和或均值时保持轴数不变\n",
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_B = A.sum(axis=1)\n",
    "sum_A, sum_B"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## keepdims的作用：\n",
    "> 如果不设置的话，A.sum(axis=1)就把A的维度由[5，4]变成了[5]\n",
    "> 而设置的话就不会把那个维度给去掉，而是变成了1，即[5，1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.0000, 0.0222, 0.0400, 0.0545],\n         [0.1000, 0.1111, 0.1200, 0.1273],\n         [0.2000, 0.2000, 0.2000, 0.2000],\n         [0.3000, 0.2889, 0.2800, 0.2727],\n         [0.4000, 0.3778, 0.3600, 0.3455]],\n\n        [[0.1429, 0.1448, 0.1467, 0.1484],\n         [0.1714, 0.1724, 0.1733, 0.1742],\n         [0.2000, 0.2000, 0.2000, 0.2000],\n         [0.2286, 0.2276, 0.2267, 0.2258],\n         [0.2571, 0.2552, 0.2533, 0.2516]]])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过广播将A除以sum_A\n",
    "A / sum_A"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.],\n         [16., 17., 18., 19.]], dtype=torch.float64),\n tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  6.,  8., 10.],\n         [12., 15., 18., 21.],\n         [24., 28., 32., 36.],\n         [40., 45., 50., 55.]], dtype=torch.float64))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 某个轴计算A元素的累计综合\n",
    "import torch\n",
    "A = torch.arange(20, dtype=float).reshape(5, 4)\n",
    "A, A.cumsum(axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import float32\n",
    "# 点积是相同位置的按元素乘积的和\n",
    "x = torch.tensor([0, 1, 2, 3], dtype=float32)\n",
    "y = torch.ones(4, dtype=float32)\n",
    "x, y, torch.dot(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(6.)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们可以通过执行按元素乘法，然后执行求和来表示两个向量的点击\n",
    "torch.sum(x * y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11],\n         [12, 13, 14, 15],\n         [16, 17, 18, 19]]),\n torch.Size([5, 4]),\n tensor([1, 2, 3, 4]),\n torch.Size([4]),\n tensor([ 20,  60, 100, 140, 180]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵向量积\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "B = torch.tensor([1, 2, 3, 4])\n",
    "A, A.shape, B, B.shape, torch.mv(A, B)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 6.,  6.,  6.],\n        [22., 22., 22.],\n        [38., 38., 38.],\n        [54., 54., 54.],\n        [70., 70., 70.]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵矩阵乘法\n",
    "A = torch.arange(20, dtype=torch.float).reshape(5, 4)\n",
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(5.)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2范数是向量元素平方和的平方根\n",
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(7.)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1范数，它表示为向量元素的绝对值之和\n",
    "torch.abs(u).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(6.)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵的F范数是矩阵元素的平方和的平方根(针对矩阵，而L2范数是针对向量）\n",
    "torch.norm(torch.ones(4, 9))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![矩阵求导](./images/image1.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![例图](./images/image3.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 向量对标量求导时，列向量的结果还是列向量\n",
    "![向量对标量求导](./images/image4.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![向量对向量求导](./images/image5.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![样例](./images/image6.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![汇总图](./images/image7.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 自动求导\n",
    "![向量链式法则](./images/image8.jpg)\n",
    "![例子1](./images/image9.jpg)\n",
    "![例子2](./images/image10.jpg)\n",
    "### 自动求导计算一个函数在指定值上的导数\n",
    "![自动求导](./images/image11.jpg)\n",
    "![计算图](./images/image12.jpg)\n",
    "![计算图](./images/image13.jpg)\n",
    "![自动求导的两种模式](./images/image14.jpg)\n",
    "![反向累积](./images/image15.jpg)\n",
    "![反向累积](./images/image16.jpg)\n",
    "![复杂度](./images/image17.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 为什么深度学习当中使用反向传播而非前向传播？\n",
    "+ 对于计算复杂度来说，正向传播和反向传播都是O(n)。\n",
    "+ 但是内存复杂度来说，反向传播因为需要保留正向传播时所有的中间结果，所以需要O(n) ,这也是神经网络特别耗GPU资源（爆显存）的祸源。\n",
    "+ 正向传播内存复杂度为O(1),但每计算一个变量的梯度都要扫一遍。\n",
    "+ 反向传播从根节点向下扫，可以保证每个节点只扫一次（在计算一个变量梯度时不用管同层的其他变量）；正向传播从叶子节点向上扫，会导致上层节点可能会计算多次。\n",
    "+ （正向中子节点比父节点先计算，因此也无法像反向那样把本节点的计算结果传给每一个子节点）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 假设我们想对函数$y=2x^Tx$关于列向量$x$求导："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 1., 2., 3.])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(4.0)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 在计算$y$关于$x$的梯度之前，我们需要一个地方来存储梯度："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "x.requires_grad_(True)\n",
    "x.grad # 默认是None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 现在让我们计算$y$："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(28., grad_fn=<MulBackward0>)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 通过调用反向传播函数来自动计算$y$关于$x$每个分量的梯度："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.,  4.,  8., 12.])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 1., 1., 1.])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在默认情况下，PyTorch会累积梯度，我们需要清楚之前的值\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 2., 4., 6.])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()\n",
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([True, True, True, True])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将某些计算移动到记录的计算图之外\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([True, True, True, True])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(True)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到变量的梯度\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "\n",
    "a.grad == d / a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-pytorch-py",
   "language": "python",
   "display_name": "Python [conda env:pytorch] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
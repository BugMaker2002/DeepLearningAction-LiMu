{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[19., 25.],\n        [37., 43.]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "K = torch.tensor([[0, 1], [2, 3]])\n",
    "corr2d(X, K)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        # 别忘了这一步\n",
    "        super().__init__()\n",
    "        # 别忘了用nn.Parameter包裹着\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.tensor([[1, -1]])\n",
    "Y = corr2d(X, K)\n",
    "Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.T, K)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2，loss为10.718\n",
      "epoch4，loss为3.592\n",
      "epoch6，loss为1.337\n",
      "epoch8，loss为0.525\n",
      "epoch10，loss为0.211\n"
     ]
    }
   ],
   "source": [
    "# 第一个1表示通道数，也就是当前层的深度为1，后面输入的图像的深度也必须跟此保持一致，为1\n",
    "# 第二个1表示输出的深度，也就是所需卷积核的个数\n",
    "# 第三个参数即卷积核的大小\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)\n",
    "# 第一个1表示一个batch中样本的个数\n",
    "# 第二个1表示通道数，也就是当前层的深度 1\n",
    "# 第三个参数是图像的高度\n",
    "# 第四个参数是图像的宽度\n",
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "# Y是输出，6=6-1+1，7=8-2+1\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "lr = 3e-2\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y - Y_hat) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f\"epoch{i + 1}，loss为{l.sum():.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.9434, -1.0377]])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 细心的读者一定会发现，我们学习到的卷积核权重非常接近我们之前定义的卷积核K\n",
    "conv2d.weight.data.reshape(1, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 8, 8])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "def comp_conv2d(conv2d, x):\n",
    "    # 因为我们之后输入的CNN是框架自带的，所以得在x前面拼接两个维度（batch_size为1和通道数为1）\n",
    "    x = x.reshape((1, 1) + x.shape)\n",
    "    y = conv2d(x)\n",
    "    return y\n",
    "# 注意，这里的padding是一侧的，而公式里面的ph是上下两侧padding加起来的大小\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n",
    "x = torch.rand(size=(8, 8))\n",
    "comp_conv2d(conv2d, x).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 8, 8])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))\n",
    "comp_conv2d(conv2d, x).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 4, 4])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, x).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 2, 2])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, x).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 多输入通道"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]]) tensor([[0, 1],\n",
      "        [2, 3]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]]) tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[ 56.,  72.],\n        [104., 120.]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corr2d_multi_in(X, K):\n",
    "    for x, k in zip(X, K):\n",
    "        print(x, k)\n",
    "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))\n",
    "# 输入X为3X3X2，K为2X2X2，输入通道数为2， 输出通道数为1\n",
    "X = torch.tensor([[[0, 1, 2], [3, 4, 5], [6, 7, 8]], [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\n",
    "K = torch.tensor([[[0, 1], [2, 3]], [[1, 2], [3, 4]]])\n",
    "corr2d_multi_in(X, K)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 多输出通道"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0, 1],\n",
      "          [2, 3]],\n",
      "\n",
      "         [[1, 2],\n",
      "          [3, 4]]],\n",
      "\n",
      "\n",
      "        [[[1, 2],\n",
      "          [3, 4]],\n",
      "\n",
      "         [[2, 3],\n",
      "          [4, 5]]],\n",
      "\n",
      "\n",
      "        [[[2, 3],\n",
      "          [4, 5]],\n",
      "\n",
      "         [[3, 4],\n",
      "          [5, 6]]]]) torch.Size([3, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 首先看一下pytorch当中函数torch.stack的用法\n",
    "# 将矩阵Z当中的每个元素加1、加2后得到一个新的矩阵后叠加到一起（在第0个维度上）\n",
    "Z = torch.tensor([[[0, 1], [2, 3]], [[1, 2], [3, 4]]])\n",
    "Z = torch.stack((Z, Z + 1, Z + 2), 0)\n",
    "print(Z, Z.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]]) tensor([[0, 1],\n",
      "        [2, 3]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]]) tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]]) tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]]) tensor([[2, 3],\n",
      "        [4, 5]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]]) tensor([[2, 3],\n",
      "        [4, 5]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]]) tensor([[3, 4],\n",
      "        [5, 6]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[ 56.,  72.],\n         [104., 120.]],\n\n        [[ 76., 100.],\n         [148., 172.]],\n\n        [[ 96., 128.],\n         [192., 224.]]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。\n",
    "def corr2d_multi_in_out(X, K):\n",
    "    # 像遍历这种多维矩阵，直接遍历都是从第0维度取矩阵\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)\n",
    "K = torch.stack((K, K + 1, K + 2), 0)\n",
    "corr2d_multi_in_out(X, K)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 用全连接层实现1X1卷积"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7902,  0.0789, -1.3491],\n",
      "        [-1.2572, -0.3948,  0.4476],\n",
      "        [ 1.6304,  1.7899, -0.6143]]) tensor([[1.1534]])\n",
      "tensor([[-1.5398,  0.1537, -0.0746],\n",
      "        [ 0.0291, -0.9116, -1.2306],\n",
      "        [ 0.1234,  1.2301,  1.4313]]) tensor([[-0.4764]])\n",
      "tensor([[ 1.4248,  0.7758,  0.7644],\n",
      "        [-1.2316,  2.6688, -1.7786],\n",
      "        [ 1.3114,  0.3418, -1.0202]]) tensor([[-0.8763]])\n",
      "tensor([[ 1.7902,  0.0789, -1.3491],\n",
      "        [-1.2572, -0.3948,  0.4476],\n",
      "        [ 1.6304,  1.7899, -0.6143]]) tensor([[-0.6258]])\n",
      "tensor([[-1.5398,  0.1537, -0.0746],\n",
      "        [ 0.0291, -0.9116, -1.2306],\n",
      "        [ 0.1234,  1.2301,  1.4313]]) tensor([[0.1809]])\n",
      "tensor([[ 1.4248,  0.7758,  0.7644],\n",
      "        [-1.2316,  2.6688, -1.7786],\n",
      "        [ 1.3114,  0.3418, -1.0202]]) tensor([[2.7515]])\n"
     ]
    }
   ],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_in, h, w = X.shape\n",
    "    c_out = K.shape[0]\n",
    "    X = X.reshape((c_in, h * w))\n",
    "    # 因为是1X1的卷积核\n",
    "    K = K.reshape((c_out, c_in))\n",
    "    Y = torch.matmul(K, X)\n",
    "    return Y.reshape((c_out, h, w))\n",
    "X = torch.normal(0, 1, (3, 3, 3))\n",
    "K = torch.normal(0, 1, (2, 3, 1, 1))\n",
    "Y1 = corr2d_multi_in_out(X, K)\n",
    "Y2 = corr2d_multi_in_out_1x1(X, K)\n",
    "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 池化层"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4., 5.],\n        [7., 8.]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pool2d(X, pool_size, mode='max'):\n",
    "    ph, pw = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - ph + 1, X.shape[1] - pw + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + ph, j: j + pw].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + ph, j: j + pw].mean()\n",
    "    return Y\n",
    "X = torch.tensor([[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]])\n",
    "pool2d(X, (2, 2), 'max')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2., 3.],\n        [5., 6.]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X, (2, 2), 'avg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 步幅和padding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[10.]]]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape(1, 1, 4, 4)\n",
    "# 使用Pytorch深度学习框架时，若未显示声明步幅，则步幅与Pooling窗口大小相同，均为3\n",
    "pooling = nn.MaxPool2d(3)\n",
    "pooling(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[ 5.,  7.],\n          [13., 15.]]]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可显示指明步幅和padding大小\n",
    "pooling2 = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pooling2(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[ 5.,  7.],\n          [13., 15.]]]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当然，我们可以设定一个任意大小的矩形汇聚窗口，并分别设定填充和步幅的高度和宽度。\n",
    "pooling3 = nn.MaxPool2d((2, 3), padding=(0, 1), stride=(2, 3))\n",
    "pooling3(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 2, 4, 4])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在处理多通道输入数据时，汇聚层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。 这意味着汇聚层的输出通道数与输入通道数相同。下面，我们将在通道维度上连结张量X和X + 1，以构建具有2个通道的输入。\n",
    "X = torch.arange(16, dtype=torch.float32).reshape(1, 1, 4, 4)\n",
    "X = torch.cat((X, X + 1), 1)\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[ 5.,  7.],\n          [13., 15.]],\n\n         [[ 6.,  8.],\n          [14., 16.]]]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling4 = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pooling4(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 下面介绍一下torch.cat()和torch.stack()的区别(cat 和 stack的区别在于 cat会增加现有维度的值,可以理解为续接，stack会新加增加一个维度，可以理解为叠加)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.torch.cat()\n",
    "#### torch.cat()函数可以将多个张量拼接成一个张量。torch.cat()有两个参数，第一个是要拼接的张量的列表或是元组；第二个参数是拼接的维度。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1.shape:  torch.Size([3, 3]) T2.shape:  torch.Size([3, 3])\n",
      "torch.Size([6, 3])\n",
      "torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# 假设是时间步T1的输出\n",
    "T1 = torch.tensor([[1, 2, 3],\n",
    "          [4, 5, 6],\n",
    "          [7, 8, 9]])\n",
    "# 假设是时间步T2的输出\n",
    "T2 = torch.tensor([[10, 20, 30],\n",
    "          [40, 50, 60],\n",
    "          [70, 80, 90]])\n",
    "print(\"T1.shape: \", T1.shape, \"T2.shape: \", T2.shape)\n",
    "print(torch.cat((T1,T2),dim=0).shape)\n",
    "print(torch.cat((T1,T2),dim=1).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.torch.stack()\n",
    "#### torch.stack()函数同样有张量列表和维度两个参数。stack与cat的区别在于，torch.stack()函数要求输入张量的大小完全相同，得到的张量的维度会比输入的张量的大小多1，并且多出的那个维度就是拼接的维度，那个维度的大小就是输入张量的个数。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1.shape:  torch.Size([3, 3]) T2.shape:  torch.Size([3, 3])\n",
      "torch.Size([2, 3, 3])\n",
      "torch.Size([3, 2, 3])\n",
      "torch.Size([3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"T1.shape: \", T1.shape, \"T2.shape: \", T2.shape)\n",
    "print(torch.stack((T1,T2),dim=0).shape)\n",
    "print(torch.stack((T1,T2),dim=1).shape)\n",
    "print(torch.stack((T1,T2),dim=2).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}